{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torchvision import transforms as tt\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport os\nfrom IPython.display import clear_output\nfrom PIL import Image\nimport shutil\nimport copy\n\n","metadata":{"id":"TidAewHk8wTG","execution":{"iopub.status.busy":"2022-07-13T12:05:32.231419Z","iopub.execute_input":"2022-07-13T12:05:32.232217Z","iopub.status.idle":"2022-07-13T12:05:32.240349Z","shell.execute_reply.started":"2022-07-13T12:05:32.232185Z","shell.execute_reply":"2022-07-13T12:05:32.238975Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install aiogram\n#!pip install qiskit ipywidgets","metadata":{"id":"GZJaN3zck8ZV","outputId":"f62a448a-dccd-42da-fb13-710c2921d2c1","execution":{"iopub.status.busy":"2022-07-13T12:05:34.642453Z","iopub.execute_input":"2022-07-13T12:05:34.645060Z","iopub.status.idle":"2022-07-13T12:05:52.925147Z","shell.execute_reply.started":"2022-07-13T12:05:34.645025Z","shell.execute_reply":"2022-07-13T12:05:52.923632Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"zJfCk0_DBj3c","execution":{"iopub.status.busy":"2022-07-12T21:19:33.461111Z","iopub.execute_input":"2022-07-12T21:19:33.461869Z","iopub.status.idle":"2022-07-12T21:19:33.475520Z","shell.execute_reply.started":"2022-07-12T21:19:33.461831Z","shell.execute_reply":"2022-07-12T21:19:33.474668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import asyncio\nfrom aiogram import Bot, Dispatcher, executor, types\nfrom aiogram.dispatcher import FSMContext\nfrom aiogram.dispatcher.filters.state import State, StatesGroup\nfrom aiogram.contrib.fsm_storage.memory import MemoryStorage\nimport nest_asyncio\n\n\nbot = Bot(token=\"5539387648:AAEe4aN8xqOsllKJdfrnxasDZt1ZdhZdwHA\")","metadata":{"id":"xpY_DaQLBEgk","execution":{"iopub.status.busy":"2022-07-13T12:05:52.932979Z","iopub.execute_input":"2022-07-13T12:05:52.933500Z","iopub.status.idle":"2022-07-13T12:05:53.322200Z","shell.execute_reply.started":"2022-07-13T12:05:52.933444Z","shell.execute_reply":"2022-07-13T12:05:53.320876Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"nest_asyncio.apply()\nstorage = MemoryStorage()\ndp = Dispatcher(bot, storage = storage)\nclass image_type(StatesGroup):\n    waiting_image_content = State()\n    waiting_image_style = State()","metadata":{"id":"U_VJs4wOtwbt","execution":{"iopub.status.busy":"2022-07-13T12:05:53.328649Z","iopub.execute_input":"2022-07-13T12:05:53.332681Z","iopub.status.idle":"2022-07-13T12:05:53.343479Z","shell.execute_reply.started":"2022-07-13T12:05:53.332627Z","shell.execute_reply":"2022-07-13T12:05:53.342076Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel1 = models.vgg16(pretrained = True).features.to(device).eval()","metadata":{"id":"T3jYrn-Z68sw","execution":{"iopub.status.busy":"2022-07-13T12:05:53.351620Z","iopub.execute_input":"2022-07-13T12:05:53.355356Z","iopub.status.idle":"2022-07-13T12:06:17.345603Z","shell.execute_reply.started":"2022-07-13T12:05:53.355303Z","shell.execute_reply":"2022-07-13T12:06:17.344362Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class StyleLoss(nn.Module):\n        def __init__(self, target_feature):\n            super(StyleLoss, self).__init__()\n            self.target = self.gram_matrix(target_feature).detach()\n            self.loss = F.mse_loss(self.target, self.target)\n\n        def forward(self, input):\n            G = self.gram_matrix(input)\n            self.loss = F.mse_loss(G, self.target)\n            return input\n\n        def gram_matrix(self,input):\n            b,c , h, w,  = input.size() \n            features = input.view(c, h*w)  \n            G = torch.mm(features, features.t())  \n            return G.div( h * w * c*b)\n            \n\nclass ContentLoss(nn.Module):\n        def __init__(self, target,):\n            super(ContentLoss, self).__init__()\n            self.target = target.detach()\n            self.loss = F.mse_loss(self.target, self.target )\n\n        def forward(self, input):\n            self.loss = F.mse_loss(input, self.target)\n            return input\n\n\nclass Normalization(nn.Module):\n        def __init__(self, mean, std):\n            super(Normalization, self).__init__()\n            self.mean = torch.tensor(mean).view(-1, 1, 1)\n            self.std = torch.tensor(std).view(-1, 1, 1)\n\n        def forward(self, img):\n            return (img - self.mean) / self.std\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-13T12:06:17.347766Z","iopub.execute_input":"2022-07-13T12:06:17.348235Z","iopub.status.idle":"2022-07-13T12:06:17.366215Z","shell.execute_reply.started":"2022-07-13T12:06:17.348188Z","shell.execute_reply":"2022-07-13T12:06:17.362949Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class TransferModel(nn.Module):\n    def __init__(self,content_layers = [ 'conv_2'],\n                 style_layers = ['conv_1', 'conv_3','conv_5','conv_9' ,'conv_13'], \n                 num_steps = 1000,\n                 content_weight = 1,\n                 style_weight = 100000\n                 ):\n       super().__init__()\n\n       self.content_layers = content_layers\n       self.style_layers = style_layers\n\n       self.content_weight = content_weight\n       self.style_weight   = style_weight\n\n       self.cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n       self.cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n       self.num_steps = num_steps\n\n    def forward(self,cnn,path, image_size):\n        content_image,style_image = self.get_photos(path = path,image_size = image_size)\n\n        input_img = content_image.clone()\n        model, style_losses, content_losses = self.get_model_and_losses(cnn,content_image,style_image)\n        output_img = self.run_style_transfer(model,input_img, content_losses,style_losses)\n        return output_img\n\n    def get_photos(self,path,image_size):\n        dir_Style_Content = path\n\n        train_im = ImageFolder(dir_Style_Content , transform = tt.Compose([\n                                                                   tt.Resize(image_size),\n                                                                   tt.CenterCrop(image_size),\n                                                                   tt.ToTensor()]))\n\n        train_im = torch.stack([i[0] for i in train_im])\n        content_img = train_im[0].view(1,*train_im[0].shape).to(device)\n        style_img   = train_im[1].view(1,*train_im[1].shape).to(device)\n        return content_img, style_img\n\n    def get_model_and_losses(self, cnn, content_img,style_img):\n\n        normalization = Normalization(self.cnn_normalization_mean, self.cnn_normalization_std).to(device)\n        cnn = copy.deepcopy(cnn)\n        content_losses = []\n        style_losses = []\n        model = nn.Sequential(normalization)\n\n        i = 0 \n        for layer in cnn.children():\n            if isinstance(layer, nn.Conv2d):\n                i += 1\n                name = 'conv_{}'.format(i)\n            elif isinstance(layer, nn.ReLU):\n                name = 'relu_{}'.format(i)\n                layer = nn.ReLU(inplace=False)\n            elif isinstance(layer, nn.MaxPool2d):\n                name = 'pool_{}'.format(i)\n            elif isinstance(layer, nn.BatchNorm2d):\n                name = 'bn_{}'.format(i)\n            else:\n                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n            model.add_module(name, layer)\n\n            if name in self.content_layers:\n              target = model(content_img).detach()\n              content_loss = ContentLoss(target)\n              model.add_module(\"content_loss_{}\".format(i), content_loss)\n              content_losses.append(content_loss)\n\n            if name in self.style_layers:\n              target_feature = model(style_img).detach()\n              style_loss = StyleLoss(target_feature)\n              model.add_module(\"style_loss_{}\".format(i), style_loss)\n              style_losses.append(style_loss)        \n\n\n        for i in range(len(model) - 1, -1, -1):\n            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n                break\n\n        model = model[:(i + 1)]\n\n        return model, style_losses, content_losses\n         \n    def run_style_transfer(self,model,input_img, content_losses, style_losses):\n        style_weight = self.style_weight\n        content_weight = self.content_weight\n \n\n        optimizer = optim.LBFGS([input_img.requires_grad_()])   \n        print('Optimizing..')\n        run = [0]\n        while run[0] <= self.num_steps:\n\n            def closure():\n                # correct the values \n                # это для того, чтобы значения тензора картинки не выходили за пределы [0;1]\n                input_img.data.clamp_(0, 1)\n\n                optimizer.zero_grad()\n\n                model(input_img)\n\n                style_score = 0\n                content_score = 0\n\n                for sl in style_losses:\n                    style_score += sl.loss\n                for cl in content_losses:\n                    content_score += cl.loss\n                \n                #взвешивание ощибки\n                style_score *= style_weight\n                content_score *= content_weight\n\n                loss = style_score + content_score\n                loss.backward()\n\n                run[0] += 1\n                if run[0] % 50 == 0:\n                    print(\"run {}:\".format(run))\n                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                        style_score.item(), content_score.item()))\n                    print()\n\n                return style_score + content_score\n\n            optimizer.step(closure)\n\n        input_img.data.clamp_(0, 1)\n\n        return input_img\n","metadata":{"id":"HT4g7Z5kszvt","execution":{"iopub.status.busy":"2022-07-13T12:06:23.739849Z","iopub.execute_input":"2022-07-13T12:06:23.740332Z","iopub.status.idle":"2022-07-13T12:06:23.779408Z","shell.execute_reply.started":"2022-07-13T12:06:23.740289Z","shell.execute_reply":"2022-07-13T12:06:23.778066Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from aiogram.types.message import ContentType\n\n\n\na = 0\n\n\n#p.message_handler(commands= 'start', state = 'cancel')\nasync def start_dialog(message: types.Message):\n          await message.reply(\" Привет.\"\n                               \"\\n\" \n                               \"\\n Отправьте первую фотографию.\"\n                               \"\\n Помните: если Вы отправляете \"\n                               \"\\n больше 2 изображений за раз,\"\n                               \"\\n то учитываться будет первая из,\"\n                               \"\\n загруженных\")  \n\n          await image_type.waiting_image_content.set()\n          await message.answer('Загрузите нужную content-картинку'\n                               '(только картинку , без текста, стикеров и т.д.) .'\n                               'Я не буду вам отвечать если вы скинете что-то другое.'\n\n                               'Отправляйте файлы как фото.  А не как документ.'\n                               'но вы можете начать диалог сначала, если введете команду /start')\n          \n\n\n#@dp.message_handler(content_types= ['photo'],state = '*')\nasync def get_content_image(message:types.Message,state : FSMContext):\n          global a\n          if a == 0:\n             a+=1\n             await message.photo[-1].download(destination_file = \"./Content_and_Style/photos/content.jpg\")\n          else: return\n\n\n          await message.reply('Выбранная фотография')       \n          #await state.update_data(chosen_content = 'content')\n          await image_type.next()\n          await message.answer('Загрузите нужную style-картинку'\n                              '(только картинку , без текста, стикеров и т.д.) .')\n          \nasync def get_style_image(message:types.Message, state:FSMContext):\n        \n          global a\n          if a == 1:\n             a+=1\n             await message.photo[-1].download(destination_file = \"./Content_and_Style/photos/style.jpg\")\n          else: return\n\n\n          await message.reply('Выбранная фотография')       \n          #await state['style'] = \n\n          await message.answer(\"All fine. Теперь ждите результирующую картинку.\")\n          model1 = models.vgg16(pretrained = True).features.to(device).eval()\n          result = TransferModel(num_steps = 1000,\n                                 content_weight = 1,\n                                 style_weight=100000000)(model1,'./Content_and_Style/',512)[0]\n          plt.figure(figsize = (20,20))\n          plt.imshow(make_grid(result).permute(1,2,0).detach().cpu().numpy())\n          plt.show() \n          result = transforms.ToPILImage()(result)\n          result.save('./Content_and_Style/photos/result.jpg')\n\n          await bot.send_photo(chat_id=message.chat.id,photo = open('./Content_and_Style/photos/result.jpg','rb'))\n          a = 0\n          os.remove('./Content_and_Style/photos/content.jpg')\n          os.remove('./Content_and_Style/photos/style.jpg')\n          os.remove('./Content_and_Style/photos/result.jpg')\n\n          await state.finish()\n\n\ndef handler_reg(dp: Dispatcher):\n     dp.register_message_handler(start_dialog, commands= 'start', state = '*')\n     dp.register_message_handler(get_content_image,content_types= \"photo\" ,state = image_type.waiting_image_content)\n     dp.register_message_handler(get_style_image, content_types='photo', state = image_type.waiting_image_style)\nhandler_reg(dp)\n\nif __name__ == \"__main__\":\n    \n    executor.start_polling(dp, skip_updates=True)\n ","metadata":{"id":"5SAPZM3xB2Rc","outputId":"678b9357-06f8-47da-efe9-1e73af75c3b1","execution":{"iopub.status.busy":"2022-07-13T12:06:26.136791Z","iopub.execute_input":"2022-07-13T12:06:26.137298Z","iopub.status.idle":"2022-07-13T12:10:10.365358Z","shell.execute_reply.started":"2022-07-13T12:06:26.137254Z","shell.execute_reply":"2022-07-13T12:10:10.364023Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"os.rmdir('./Content_and_Style/photos')\n#os.remove('./Content_and_Style/photos/result.jpg')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-12T21:57:17.841112Z","iopub.execute_input":"2022-07-12T21:57:17.841489Z","iopub.status.idle":"2022-07-12T21:57:17.859721Z","shell.execute_reply.started":"2022-07-12T21:57:17.841458Z","shell.execute_reply":"2022-07-12T21:57:17.858522Z"},"trusted":true},"execution_count":null,"outputs":[]}]}